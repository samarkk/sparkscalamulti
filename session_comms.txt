# open the terminal
# and just punch in
spark-shell
just an illustration
https://github.com/samarkk/scalaproject
val playersMatchesScores = List(("sachin",102,20545), ("virat",85,15456))
val prdd = sc.parallelize(playersMatchesScores)
prdd.map(x  => (x._1, (x._2, x._3))).collect
https://www.gutenberg.org/ebooks/100
val shakRDD = sc.textFile("C:/data/shakespeare.txt")
shakRDD.take(5)
val shakWords = shakRDD.flatMap(x => x.split(" "))
val shakWordsFiltered = shakWords.filter(x => x != "")
val shakTuples = shakWordsFiltered.map(x => (x, 1))
val shakCounts = shakTuples.reduceByKey(_ + _)
val shakCountsSorted = shakCounts.sortBy(x => -x._2)
shakCountsSorted.take(20).foreach(println)
sc.textFile("C:/data/shakespeare.txt").flatMap(x => x.split(" ")).filter(x => x != "").map(x => (x, 1)).reduceByKey((x,y) => x + y).sortBy(x => - x._2).take(20).foreach(println)
sc.textFile("C:/data/shakespeare.txt").flatMap(_.split(" ")).filter(_ != "").map(x => (x.toLowerCase, 1)).reduceByKey(_ + _).sortBy(- _._2).take(20).foreach(println)
cd \sparkmulti
vagrant up master node1 node2 node3
ls -l /home/vagrant/c/data
hdfs dfs -put /home/vagrant/c/data/apachelogs.gz 
hdfs dfs -put /home/vagrant/c/data/apachelogs50k.gz

