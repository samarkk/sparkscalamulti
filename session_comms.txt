# open the terminal
# and just punch in
spark-shell
just an illustration
https://github.com/samarkk/scalaproject
val playersMatchesScores = List(("sachin",102,20545), ("virat",85,15456))
val prdd = sc.parallelize(playersMatchesScores)
prdd.map(x  => (x._1, (x._2, x._3))).collect
https://www.gutenberg.org/ebooks/100
val shakRDD = sc.textFile("C:/data/shakespeare.txt")
shakRDD.take(5)
val shakWords = shakRDD.flatMap(x => x.split(" "))
val shakWordsFiltered = shakWords.filter(x => x != "")
val shakTuples = shakWordsFiltered.map(x => (x, 1))
val shakCounts = shakTuples.reduceByKey(_ + _)
val shakCountsSorted = shakCounts.sortBy(x => -x._2)
shakCountsSorted.take(20).foreach(println)
sc.textFile("C:/data/shakespeare.txt").flatMap(x => x.split(" ")).filter(x => x != "").map(x => (x, 1)).reduceByKey((x,y) => x + y).sortBy(x => - x._2).take(20).foreach(println)
sc.textFile("C:/data/shakespeare.txt").flatMap(_.split(" ")).filter(_ != "").map(x => (x.toLowerCase, 1)).reduceByKey(_ + _).sortBy(- _._2).take(20).foreach(println)
cd \sparkmulti
vagrant up master node1 node2 node3
ls -l /home/vagrant/c/data
hdfs dfs -mkdir -p /user/vagrant
hdfs dfs -put /home/vagrant/c/data/apachelogs.gz 
hdfs dfs -put /home/vagrant/c/data/apachelogs50k.gz
https://github.com/samarkk/scalaproject/blob/master/src/main/scala/patternmatching/PatternMatchingExample.scala
val adf = accessLogs.toDF
adf.printSchema
adf.groupBy("responseCode").count()
adf.createOrReplaceTempView("adt")
spark.sql("Select responsecode, count(*) from adt group by responsecode").show
spark-shell --master yarn --executor-memory 1500M --conf spark.memory.fraction=0.8 --num-executors 4 --executor-cores 1
libraryDependencies ++= Seq("org.apache.spark" %% "spark-core" % "3.3.0",
  "org.apache.spark" %% "spark-sql" % "3.3.0")
import org.apache.spark.sql.SparkSession
val spark = SparkSession.builder().appName("DemoSpark").getOrCreate()
println(spark.version)
val sc = spark.sparkContext
val ardd = sc.parallelize(1 to 1000)
println(ardd.sum())
 hive --service metastore 2>&1 &>/dev/null &
 tlpg 9083
 hive
 create table ftbl(c1 integer, c2 string);
 insert into ftbl values (1, 'ram'), (2, 'sham'), (3, 'sita'), (4, 'gita');
show tables;
 select * from ftbl;
 dfs -ls /user/hive;
 dfs -mkkdir eftbldir;
 dfs -cp /user/hive/warehouse/ftbl/00* eftbldir;
 create external table eftbl(c1 integer, c2 string) location 'hdfs://192.168.56.2:8020/user/vagrant/eftbldir';
show tables;
select * from eftbl;
drop table ftbl;
drop table eftbl;
dfs -ls;
dfs -ls -R /user/hive;
https://github.com/samarkk/spark2project/tree/master/src/main/scala/optimizationsandfindings
https://github.com/samarkk/kafka-examples/blob/main/src/main/scripts/kafka_course_3.txt
